{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2500d96",
   "metadata": {},
   "source": [
    "# üìö GIKI Prospectus Q&A Chatbot (RAG)\n",
    "Bilingual chatbot (English + Urdu) using FAISS + MiniLM embeddings + Flan-T5-Small (direct generation for stability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ddf787",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip -q install pymupdf python-docx faiss-cpu sentence-transformers transformers gradio deep-translator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35541e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, re, fitz, docx\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from deep_translator import GoogleTranslator\n",
    "import gradio as gr\n",
    "import torch\n",
    "\n",
    "embedder = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "def translate_to_en(text):\n",
    "    try:\n",
    "        return GoogleTranslator(source='auto', target='en').translate(text)\n",
    "    except:\n",
    "        return text\n",
    "\n",
    "def translate_to_ur(text):\n",
    "    try:\n",
    "        return GoogleTranslator(source='en', target='ur').translate(text)\n",
    "    except:\n",
    "        return text\n",
    "\n",
    "def answer_with_t5(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=80)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140c440f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_text_from_file(file):\n",
    "    if file.name.endswith('.pdf'):\n",
    "        doc = fitz.open(file.name)\n",
    "        text = \"\"\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "        return text\n",
    "    elif file.name.endswith('.docx'):\n",
    "        d = docx.Document(file.name)\n",
    "        return \"\\n\".join([p.text for p in d.paragraphs])\n",
    "    elif file.name.endswith('.txt'):\n",
    "        return open(file.name, encoding='utf-8').read()\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def chunk_text(text, max_tokens=400):\n",
    "    sentences = re.split(r'(?<=[.!?]) +', text)\n",
    "    chunks, current = [], \"\"\n",
    "    for s in sentences:\n",
    "        if len((current+s).split()) > max_tokens:\n",
    "            chunks.append(current.strip())\n",
    "            current = s\n",
    "        else:\n",
    "            current += \" \" + s\n",
    "    if current:\n",
    "        chunks.append(current.strip())\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59206612",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_faiss_index(docs):\n",
    "    texts, meta = [], []\n",
    "    for file in docs:\n",
    "        t = extract_text_from_file(file)\n",
    "        ch = chunk_text(t)\n",
    "        texts.extend(ch)\n",
    "        meta.extend([file.name]*len(ch))\n",
    "    embeds = embedder.encode(texts, convert_to_numpy=True)\n",
    "    dim = embeds.shape[1]\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(embeds)\n",
    "    return index, texts, meta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f663d2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rag_answer(query, index, texts, meta, top_k=2, lang='en'):\n",
    "    q_orig = query\n",
    "    q_en = translate_to_en(query) if lang=='ur' else query\n",
    "    q_emb = embedder.encode([q_en], convert_to_numpy=True)\n",
    "    D,I = index.search(q_emb, top_k)\n",
    "    context = \" \".join([texts[i][:300] for i in I[0]])\n",
    "    prompt = f\"Context: {context}\\n\\nQuestion: {q_en}\\nAnswer:\"\n",
    "    try:\n",
    "        ans = answer_with_t5(prompt)\n",
    "    except Exception as e:\n",
    "        ans = f\"‚ö†Ô∏è Model error: {e}\"\n",
    "    if lang=='ur' or (q_orig != q_en):\n",
    "        ans = translate_to_ur(ans)\n",
    "    return ans, [meta[i] for i in I[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9054909c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "docs, texts, meta, index = [], [], [], None\n",
    "\n",
    "def upload_files(files):\n",
    "    global docs, texts, meta, index\n",
    "    docs = files\n",
    "    index, texts, meta = build_faiss_index(docs)\n",
    "    return f\"Uploaded {len(docs)} documents, processed into {len(texts)} chunks.\"\n",
    "\n",
    "def chatbot(query, lang_choice):\n",
    "    if not index:\n",
    "        return \"Please upload documents first.\", []\n",
    "    ans, sources = rag_answer(query, index, texts, meta, lang='ur' if lang_choice=='Urdu' else 'en')\n",
    "    return ans, sources\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## üìò GIKI Prospectus RAG Chatbot (English + Urdu)\")\n",
    "    with gr.Row():\n",
    "        uploader = gr.File(file_types=['.pdf','.docx','.txt'], file_count=\"multiple\", label=\"Upload Documents\")\n",
    "    status = gr.Textbox(label=\"Status\")\n",
    "    uploader.upload(upload_files, uploader, status)\n",
    "    with gr.Row():\n",
    "        query = gr.Textbox(label=\"Your Question (English or Urdu)\")\n",
    "        lang = gr.Radio([\"English\",\"Urdu\"], value=\"English\", label=\"Answer Language\")\n",
    "    btn = gr.Button(\"Ask\")\n",
    "    output = gr.Textbox(label=\"Answer\")\n",
    "    sources = gr.Label(label=\"Sources\")\n",
    "    btn.click(fn=chatbot, inputs=[query, lang], outputs=[output, sources])\n",
    "demo.launch(share=True)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
